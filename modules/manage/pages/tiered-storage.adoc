= Tiered Storage
:description: Configure your Redpanda cluster running on Linux to offload log segments to cloud storage and save storage costs.
:contextLinks: [{"name"=>"Linux", "to"=>"manage/tiered-storage"}, {"name"=>"Kubernetes", "to"=>"manage/kubernetes/tiered-storage"}]
:deployment: Linux
:linkRoot: ../../

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import ClusterProperty from './shared/tiered-storage/_cluster-props.mdx'
import TunableClusterProperty from './shared/tiered-storage/_tunable-cluster-props.mdx'
import NodeProperty from './shared/tiered-storage/_node-props.mdx'
import EnterpriseLicenseNote from '@site/docs/shared/_enterprise-license.mdx'
import AboutTieredStorage from './shared/tiered-storage/_intro.mdx';
import SetUp from './shared/tiered-storage/_set-up.mdx';
import Enable from './shared/tiered-storage/_enable.mdx';
import SetRetentionLimits from './shared/tiered-storage/_set-retention-limits.mdx';
import RemoteWrite from './shared/tiered-storage/_remote-write.mdx';
import IdleTimeout from './shared/tiered-storage/_idle-timeout.mdx';
import Reconciliation from './shared/tiered-storage/_reconciliation.mdx';
import UploadController from './shared/tiered-storage/_upload-controller.mdx';
import RemoteRead from './shared/tiered-storage/_remote-read.mdx';
import RemoteRecovery from './shared/tiered-storage/_remote-recovery.mdx';
import RetriesAndBackoff from './shared/tiered-storage/_retries-and-backoff.mdx';
import Transport from './shared/tiered-storage/_transport.mdx';
import ConfigurationProperties from './shared/tiered-storage/_configuration-properties.mdx';
import ContextLink from '@site/src/components/ContextButton.js'

<ContextLink frontmatter=\{frontMatter}
/>

<AboutTieredStorage frontmatter=\{frontMatter}/>

== Prerequisites

<EnterpriseLicenseNote frontmatter=\{frontMatter}/>

== Set Up Tiered Storage+++<SetUp>++++++</SetUp>+++

=== Configure cloud storage

////
[tabs]
=====
Amazon S3::
+
--
Configure access to Amazon S3 with either an IAM role attached to the instance or with access keys.

To configure access to an S3 bucket with an IAM role:

. Configure an xref:manage:security:iam-roles:.adoc#configuring-iam-roles[IAM role].
. Run the `rpk cluster config edit` command, then edit the following required properties:
+
[,properties]
----
cloud_storage_enabled: true
cloud_storage_credentials_source: <aws_instance_metadata>
cloud_storage_region: <region>
cloud_storage_bucket: <redpanda_bucket_name>
----
+
Replace `<variables>` with your own values.

To configure access to an S3 bucket with access keys instead of an IAM role:

. Grant a user the following permissions to read and create objects on the bucket to be used with the cluster (or on all buckets): `GetObject`, `DeleteObject`, `PutObject`, `PutObjectTagging`, `ListBucket`.
. Copy the access key and secret key for the `cloud_storage_access_key` and `cloud_storage_secret_key` cluster properties.
. Run the `rpk cluster config edit` command, then edit the following required properties:
+
[,properties]
----
cloud_storage_enabled: true
cloud_storage_access_key: ***
cloud_storage_secret_key: ***
cloud_storage_region: <region>
cloud_storage_bucket: <redpanda_bucket_name>
----
+
Replace `<variables>` with your own values.

For additional properties, see <<tiered-storage-configuration-properties,Tiered Storage configuration properties>>.

--
Google Cloud Storage::
+
--
Configure access to Google Cloud Storage with either an IAM role attached to the instance or with access keys.

To configure access to Google Cloud Storage with an IAM role:

. Configure an xref:manage:security:iam-roles:.adoc#configuring-iam-roles[IAM role].
. Run the `rpk cluster config edit` command, then edit the following required properties:
+
[,properties]
----
cloud_storage_enabled: true
cloud_storage_credentials_source: <gcp_instance_metadata>
cloud_storage_region: <region>
cloud_storage_bucket: <redpanda_bucket_name>
----
+
Replace `<variables>` with your own values.

To configure access to Google Cloud Storage with access keys instead of an IAM role:

. Choose a uniform access control when you create the bucket.
. Use a Google-managed encryption key.
. Set a https://cloud.google.com/storage/docs/migrating#defaultproj[default project].
. Create a service user with https://cloud.google.com/storage/docs/authentication/managing-hmackeys[HMAC keys] and copy the access key and secret key for the `cloud_storage_access_key` and `cloud_storage_secret_key` cluster properties.
. Run the `rpk cluster config edit` command, then edit the following required properties:
+
[,properties]
----
cloud_storage_enabled: true
cloud_storage_access_key: ***
cloud_storage_api_endpoint: storage.googleapis.com
cloud_storage_bucket: <redpanda_bucket_name>
cloud_storage_region: <region>
cloud_storage_secret_key: ***
----
+
Replace `<variables>` with your own values.

For additional properties, see <<tiered-storage-configuration-properties,Tiered Storage configuration properties>>.

--
=====
////

=== Enable Tiered Storage

<Enable frontmatter=\{frontMatter}/>

=== Set retention limits

<SetRetentionLimits frontmatter=\{frontMatter}/>

== Remote write

<RemoteWrite frontmatter=\{frontMatter}/>

=== Idle timeout

<IdleTimeout frontmatter=\{frontMatter}/>

=== Reconciliation

<Reconciliation frontmatter=\{frontMatter}/>

=== Upload controller

<UploadController frontmatter=\{frontMatter}/>

== Remote read

<RemoteRead frontmatter=\{frontMatter}/>

=== Caching

When the Kafka client fetches an offset range that isn't available locally in the Redpanda data directory, Redpanda downloads remote segments from cloud storage. These downloaded segments are stored in the cloud storage cache directory. The cache directory is created in the Redpanda `data` directory by default, but it can be placed anywhere in the system. For example, you might want to put the cache directory to a dedicated drive with cheaper storage.

If you don't specify a cache location in the `redpanda.yaml` file, the cache directory is created in `<redpanda_data_directory>/cloud_storage_cache`.

Use the <NodeProperty name="cloud_storage_cache_directory" frontmatter=\{frontMatter}/>  property on each broker to specify a different location for the cache directory. You must specify the full path.

Redpanda checks the cache periodically, and if the size of the stored data is larger than the configured limit, the eviction process starts. The eviction process removes segments that haven't been accessed recently, until the size of the cache drops 20%.

Use the following cluster-level properties to set the maximum cache size and cache check interval. Edit them with the `rpk cluster config edit` command.

|===
| Property | Description

| <ClusterProperty name="cloud_storage_cache_size" frontmatter=\{frontMatter}/>
| Maximum size of the disk cache used by Tiered Storage. Default is 20 GiB.

| <TunableClusterProperty name="cloud_storage_cache_check_interval" frontmatter=\{frontMatter}/>
| The time, in milliseconds, between cache checks. The size of the cache can grow quickly, so it's important to have a small interval between checks. However, if the checks are too frequent, they consume a lot of resources. Default is 30000 ms.
|===

== Remote recovery

<RemoteRecovery frontmatter=\{frontMatter}/>

== Retries and backoff

<RetriesAndBackoff frontmatter=\{frontMatter}/>

== Transport

<Transport frontmatter=\{frontMatter}/>

== Tiered Storage configuration properties

<ConfigurationProperties frontmatter=\{frontMatter}/>

== Suggested reading

* https://redpanda.com/blog/tiered-storage-architecture-shadow-indexing-deep-dive/[How we built shadow indexing, the subsystem powering Tiered Storage]
* xref:cluster-maintenance:cluster-property-configuration.adoc[Configure Cluster Properties]
